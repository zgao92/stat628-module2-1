#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Feb 17 15:27:27 2019

@author: xiajian
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from nltk.tokenize import RegexpTokenizer
from nltk.stem.snowball import SnowballStemmer

train = pd.read_csv('train.csv',nrows=10000) #this take a while
train.head(5)
list(train.columns.values)

#get rid of duplicate data by description column
train[train.duplicated('text',keep=False)].sort_values('text').head(5)
train = train.drop_duplicates('text')

#natural language process
#use the English stop words list that is defaulted in the sklearn library
punc = ['.', ',', '"', "'", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',"%"]
stop_words = text.ENGLISH_STOP_WORDS.union(punc)
desc = train['text'].values
y=train['stars']

#make text as vector
#different form of word show up
vectorizer = TfidfVectorizer(stop_words = stop_words)
X = vectorizer.fit_transform(desc)

word_features = vectorizer.get_feature_names()
word_features[550:575]

# only the word root form is present
stemmer = SnowballStemmer('english')
tokenizer = RegexpTokenizer(r'[a-zA-Z\']+')

def tokenize(text):
    return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())]

vectorizer2 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize)
X2 = vectorizer2.fit_transform(desc)
word_features2 = vectorizer2.get_feature_names()
word_features2[:50]

#limited our token feature space with regular expressions,decrease the dimension of vector
#selects only the top max_features tokens ordered by their frequencies in the corpus to be included in the vectorizing.
vectorizer3 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize, max_features = 100)
X3 = vectorizer3.fit_transform(desc)
words = vectorizer3.get_feature_names()


